{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f8d36d",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f12a45a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T12:53:12.043999Z",
     "start_time": "2022-06-07T12:53:11.982975Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy # install command: pip install spacy\n",
    "import string\n",
    "import nltk # install command: pip install nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import onnxruntime # install command: pip install onnxruntime\n",
    "import torch.onnx as onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6d5226a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T12:53:12.790788Z",
     "start_time": "2022-06-07T12:53:12.786937Z"
    }
   },
   "outputs": [],
   "source": [
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3dbbc15b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T12:53:13.332616Z",
     "start_time": "2022-06-07T12:53:13.325569Z"
    }
   },
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9a01d18d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T12:53:14.178352Z",
     "start_time": "2022-06-07T12:53:14.174293Z"
    }
   },
   "outputs": [],
   "source": [
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2ae09d3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T12:53:15.087167Z",
     "start_time": "2022-06-07T12:53:15.071639Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'max_features': 1000,\n",
    "    'num_epochs':200,\n",
    "    'learning_rate':1e-1,\n",
    "    'batch_size':32,\n",
    "    'train_percentage':90\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8317505",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6fbc2a",
   "metadata": {},
   "source": [
    "Note that the dataset is taken from https://www.kaggle.com/datasets/yasserh/twitter-tweets-sentiment-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ded9e",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c94cc999",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T12:54:13.585581Z",
     "start_time": "2022-06-07T12:54:13.466482Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d2360b30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T12:55:16.030122Z",
     "start_time": "2022-06-07T12:55:15.999686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'negative', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "87fb728c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T12:55:37.625606Z",
     "start_time": "2022-06-07T12:55:37.584586Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.loc[df['sentiment']!='neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7c1824c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T12:55:52.530116Z",
     "start_time": "2022-06-07T12:55:52.501840Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "1      Sooo SAD I will miss you here in San Diego!!!  negative\n",
       "2                          my boss is bullying me...  negative\n",
       "3                     what interview! leave me alone  negative\n",
       "4   Sons of ****, why couldn`t they put them on t...  negative\n",
       "6  2am feedings for the baby are fun when he is a...  positive"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['text','sentiment']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f34a98e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T12:56:06.896776Z",
     "start_time": "2022-06-07T12:56:06.877288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16363"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22995c40",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89835b9d",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1d8ff9a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T12:58:23.801740Z",
     "start_time": "2022-06-07T12:58:23.792486Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8222a654",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T12:58:33.702709Z",
     "start_time": "2022-06-07T12:58:33.688971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setting'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"settings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631ef75",
   "metadata": {},
   "source": [
    "### Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "32254da3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T12:59:45.630515Z",
     "start_time": "2022-06-07T12:59:44.892831Z"
    }
   },
   "outputs": [],
   "source": [
    "en = spacy.load('en_core_web_sm')\n",
    "stopwords = en.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1bbfe590",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:00:04.741846Z",
     "start_time": "2022-06-07T13:00:04.733390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "72beacae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:00:06.931762Z",
     "start_time": "2022-06-07T13:00:06.926317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cc2b505b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:00:32.026822Z",
     "start_time": "2022-06-07T13:00:32.019442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['elsewhere',\n",
       " 'sixty',\n",
       " 'by',\n",
       " 'toward',\n",
       " 'cannot',\n",
       " 'perhaps',\n",
       " 'over',\n",
       " 'down',\n",
       " 'any',\n",
       " 'thence']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(stopwords)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2819073b",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aefb6702",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:03:28.469718Z",
     "start_time": "2022-06-07T13:03:28.448570Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    txt = txt.lower()\n",
    "    tokens = txt.split()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    txt = ' '.join(tokens)\n",
    "    txt = txt.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = txt.split()\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    txt = ' '.join(tokens)\n",
    "    txt = re.sub(r'[0-9]+', '', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4be1bc5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:04:33.152749Z",
     "start_time": "2022-06-07T13:04:33.134307Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.reset_index() # restore the indices of the dataframe so that it starts from 0 and skips nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8cd240ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:04:34.320488Z",
     "start_time": "2022-06-07T13:04:34.306446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original text was:\n",
      " Well what im working on isn`t QUITE ready to post about publicly (still beta testing) but its a cool new script I coded\n",
      " The preprocessed text is: \n",
      "im working isnt ready post publicly beta testing cool new script coded\n"
     ]
    }
   ],
   "source": [
    "original_txt = df['text'][50]\n",
    "processed_txt = preprocess(df['text'][50])\n",
    "print(f'The original text was:\\n{original_txt}\\n The preprocessed text is: \\n{processed_txt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c4560071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:06:10.429918Z",
     "start_time": "2022-06-07T13:06:09.580485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>sooo sad miss san diego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>bos bullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>interview leave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>son couldnt release bought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>am feeding baby fun smile coo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text sentiment  \\\n",
       "0      1      Sooo SAD I will miss you here in San Diego!!!  negative   \n",
       "1      2                          my boss is bullying me...  negative   \n",
       "2      3                     what interview! leave me alone  negative   \n",
       "3      4   Sons of ****, why couldn`t they put them on t...  negative   \n",
       "4      6  2am feedings for the baby are fun when he is a...  positive   \n",
       "\n",
       "               preprocessed_text  \n",
       "0        sooo sad miss san diego  \n",
       "1                   bos bullying  \n",
       "2                interview leave  \n",
       "3     son couldnt release bought  \n",
       "4  am feeding baby fun smile coo  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['preprocessed_text'] = df['text'].apply(lambda x: preprocess(str(x)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4f6d45d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:06:34.307461Z",
     "start_time": "2022-06-07T13:06:34.284354Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = df['preprocessed_text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f8ff8ab4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:06:43.595751Z",
     "start_time": "2022-06-07T13:06:43.384307Z"
    }
   },
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(max_features=config['max_features'])\n",
    "# features = vectorizer.fit_transform(texts)\n",
    "vectorizer = TfidfVectorizer(max_features=config['max_features'])\n",
    "features = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e03cd4b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:07:54.825763Z",
     "start_time": "2022-06-07T13:07:54.802178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['able', 'absolutely', 'account', 'ache', 'actually', 'add',\n",
       "       'afford', 'afraid', 'afternoon', 'age', 'ago', 'agree', 'ah',\n",
       "       'ahh', 'ahhh', 'aint', 'air', 'album', 'alot', 'alright', 'am',\n",
       "       'amazing', 'america', 'american', 'annoying', 'answer', 'anymore',\n",
       "       'anyways', 'apart', 'app', 'apparently', 'apple', 'appreciate',\n",
       "       'arent', 'arm', 'art', 'ask', 'asking', 'asleep', 'ate', 'aw',\n",
       "       'awake', 'away', 'awesome', 'awful', 'aww', 'awww', 'awwww',\n",
       "       'babe', 'baby', 'bad', 'bag', 'ball', 'band', 'bank', 'bar',\n",
       "       'barely', 'bbq', 'bc', 'bday', 'beach', 'beat', 'beautiful', 'bed',\n",
       "       'beer', 'believe', 'ben', 'best', 'bet', 'better', 'bgt', 'big',\n",
       "       'bike', 'bird', 'birthday', 'bit', 'black', 'blackberry', 'blah',\n",
       "       'blast', 'bless', 'blessed', 'block', 'blog', 'blood', 'bloody',\n",
       "       'blue', 'body', 'boo', 'book', 'bored', 'boring', 'bought', 'bout',\n",
       "       'boy', 'boyfriend', 'break', 'breakfast', 'breaking', 'bring',\n",
       "       'britain', 'bro', 'broke', 'broken', 'brother', 'brought', 'btw',\n",
       "       'bug', 'bummed', 'bummer', 'burn', 'burned', 'burnt', 'bus',\n",
       "       'business', 'busy', 'buy', 'bye', 'cake', 'called', 'came',\n",
       "       'camera', 'cancelled', 'cant', 'car', 'card', 'care', 'case',\n",
       "       'cat', 'catch', 'cause', 'cd', 'chance', 'change', 'chat', 'check',\n",
       "       'checked', 'cheer', 'cheese', 'chicken', 'child', 'chill',\n",
       "       'chillin', 'chocolate', 'choice', 'church', 'city', 'class',\n",
       "       'clean', 'cleaning', 'close', 'clothes', 'club', 'co', 'coffee',\n",
       "       'cold', 'college', 'come', 'coming', 'comment', 'company',\n",
       "       'completely', 'computer', 'concert', 'congrats', 'congratulation',\n",
       "       'congratulations', 'cook', 'cool', 'copy', 'cost', 'couldnt',\n",
       "       'couple', 'course', 'cousin', 'cover', 'coz', 'crazy', 'cream',\n",
       "       'cried', 'cry', 'cup', 'currently', 'cut', 'cute', 'cuz', 'da',\n",
       "       'dad', 'daddy', 'dammit', 'dance', 'dancing', 'dang', 'dark',\n",
       "       'darn', 'date', 'daughter', 'david', 'day', 'days', 'dead', 'deal',\n",
       "       'dear', 'death', 'decided', 'def', 'definitely', 'degree',\n",
       "       'delicious', 'depressed', 'depressing', 'deserve', 'design',\n",
       "       'didnt', 'die', 'died', 'different', 'dinner', 'disappointed',\n",
       "       'dnt', 'doe', 'doesnt', 'dog', 'dont', 'dream', 'dress', 'drink',\n",
       "       'drinking', 'drive', 'driving', 'drunk', 'dude', 'dvd', 'earlier',\n",
       "       'early', 'earth', 'easy', 'eat', 'eating', 'eh', 'em', 'email',\n",
       "       'end', 'english', 'enjoy', 'enjoyed', 'enjoying', 'entire', 'epic',\n",
       "       'episode', 'especially', 'etc', 'evening', 'event', 'everybody',\n",
       "       'everyday', 'exactly', 'exam', 'excellent', 'excited', 'exciting',\n",
       "       'exhausted', 'expensive', 'experience', 'extra', 'eye', 'fab',\n",
       "       'face', 'facebook', 'fact', 'fail', 'failed', 'fair', 'fall',\n",
       "       'falling', 'fam', 'family', 'fan', 'fantastic', 'far', 'fast',\n",
       "       'fav', 'fave', 'favorite', 'favourite', 'fb', 'feel', 'feelin',\n",
       "       'feeling', 'fell', 'felt', 'fever', 'ff', 'fight', 'figure',\n",
       "       'film', 'final', 'finally', 'find', 'fine', 'finger', 'finish',\n",
       "       'finished', 'fit', 'fix', 'flight', 'flower', 'flu', 'fly', 'fml',\n",
       "       'follow', 'followed', 'follower', 'followers', 'followfriday',\n",
       "       'following', 'food', 'foot', 'forever', 'forget', 'forgot',\n",
       "       'forward', 'found', 'freakin', 'freaking', 'free', 'french',\n",
       "       'fresh', 'friday', 'friend', 'friends', 'fun', 'funny', 'future',\n",
       "       'game', 'garden', 'gave', 'germany', 'gettin', 'getting', 'gift',\n",
       "       'gig', 'girl', 'girls', 'giving', 'glad', 'god', 'goin', 'going',\n",
       "       'gone', 'gonna', 'good', 'goodbye', 'goodnight', 'google',\n",
       "       'gorgeous', 'gosh', 'got', 'gotta', 'graduation', 'great', 'green',\n",
       "       'group', 'guess', 'gutted', 'guy', 'guys', 'gym', 'ha', 'haha',\n",
       "       'hahah', 'hahaha', 'hahahaha', 'hair', 'half', 'hand', 'hang',\n",
       "       'hanging', 'hannah', 'happen', 'happened', 'happens', 'happiness',\n",
       "       'happy', 'hard', 'hate', 'havent', 'having', 'head', 'headache',\n",
       "       'heading', 'healthy', 'hear', 'heard', 'heart', 'heat', 'hehe',\n",
       "       'hell', 'hello', 'help', 'helping', 'hes', 'hey', 'hi', 'high',\n",
       "       'hilarious', 'hill', 'history', 'hit', 'hmm', 'holiday', 'home',\n",
       "       'homework', 'hope', 'hopefully', 'hoping', 'horrible', 'hospital',\n",
       "       'hot', 'hour', 'hours', 'house', 'hr', 'hubby', 'hug', 'huge',\n",
       "       'hugs', 'huh', 'hun', 'hungry', 'hurt', 'hurting', 'hurts', 'ice',\n",
       "       'id', 'idea', 'idk', 'ill', 'im', 'ima', 'important', 'impressed',\n",
       "       'info', 'inside', 'instead', 'interesting', 'internet',\n",
       "       'interview', 'iphone', 'ipod', 'isnt', 'itll', 'itunes', 'ive',\n",
       "       'iï', 'jealous', 'job', 'john', 'join', 'joke', 'jon', 'jonas',\n",
       "       'joy', 'july', 'june', 'jus', 'justin', 'key', 'kick', 'kid',\n",
       "       'kids', 'kill', 'killed', 'killing', 'kind', 'kinda', 'kiss',\n",
       "       'knee', 'knew', 'know', 'la', 'lady', 'lake', 'lame', 'laptop',\n",
       "       'late', 'lately', 'later', 'laugh', 'lay', 'laying', 'lazy', 'le',\n",
       "       'learn', 'leave', 'leaving', 'left', 'leg', 'let', 'lets', 'life',\n",
       "       'light', 'like', 'liked', 'lil', 'line', 'link', 'list', 'listen',\n",
       "       'listening', 'little', 'live', 'living', 'lmao', 'load', 'lol',\n",
       "       'london', 'lonely', 'long', 'longer', 'look', 'looked', 'looking',\n",
       "       'lose', 'losing', 'lost', 'lot', 'love', 'loved', 'lovely',\n",
       "       'loving', 'low', 'luck', 'lucky', 'lunch', 'luv', 'ma', 'mac',\n",
       "       'macbook', 'mad', 'magic', 'making', 'mama', 'man', 'mate', 'math',\n",
       "       'matter', 'maybe', 'mean', 'meant', 'meet', 'meeting', 'men',\n",
       "       'mention', 'mess', 'message', 'messed', 'met', 'middle', 'mile',\n",
       "       'miley', 'min', 'mind', 'minute', 'miss', 'missed', 'missing',\n",
       "       'mobile', 'mom', 'moment', 'momma', 'mommy', 'moms', 'monday',\n",
       "       'money', 'month', 'months', 'mood', 'moon', 'morning', 'mother',\n",
       "       'mothers', 'mouth', 'moved', 'movie', 'moving', 'mr', 'mum',\n",
       "       'music', 'nap', 'nd', 'near', 'neck', 'need', 'needed', 'new',\n",
       "       'news', 'nice', 'night', 'nite', 'nope', 'nose', 'note', 'number',\n",
       "       'ny', 'nyc', 'office', 'officially', 'oh', 'ohh', 'ohhh', 'ok',\n",
       "       'okay', 'old', 'omg', 'ones', 'online', 'ooh', 'oops', 'open',\n",
       "       'order', 'ouch', 'outside', 'outta', 'packing', 'page', 'pain',\n",
       "       'paper', 'parent', 'park', 'party', 'pas', 'past', 'pay', 'pc',\n",
       "       'peace', 'people', 'perfect', 'person', 'phone', 'photo', 'pic',\n",
       "       'pick', 'picture', 'pink', 'pissed', 'pizza', 'place', 'plan',\n",
       "       'play', 'played', 'playing', 'plus', 'pm', 'point', 'pool', 'poor',\n",
       "       'post', 'posted', 'power', 'ppl', 'pray', 'present', 'pretty',\n",
       "       'prob', 'probably', 'problem', 'productive', 'profile', 'project',\n",
       "       'prom', 'proud', 'ps', 'public', 'puppy', 'putting', 'question',\n",
       "       'quick', 'quiet', 'quote', 'radio', 'rain', 'raining', 'rainy',\n",
       "       'ran', 'read', 'reading', 'ready', 'real', 'realized', 'reason',\n",
       "       'red', 'relaxing', 'remember', 'reply', 'rest', 'return', 'review',\n",
       "       'revision', 'ride', 'right', 'ring', 'rip', 'rock', 'room',\n",
       "       'round', 'run', 'running', 'sad', 'sadly', 'safe', 'said', 'sale',\n",
       "       'sam', 'sat', 'saturday', 'save', 'saw', 'saying', 'scared',\n",
       "       'scary', 'school', 'screen', 'season', 'second', 'seeing', 'seen',\n",
       "       'self', 'send', 'sending', 'sense', 'sent', 'series', 'seriously',\n",
       "       'service', 'session', 'set', 'sexy', 'shall', 'shame', 'share',\n",
       "       'sharing', 'shes', 'shift', 'shirt', 'shoe', 'shop', 'shopping',\n",
       "       'short', 'shot', 'shouldnt', 'shower', 'showing', 'shut', 'si',\n",
       "       'sick', 'sigh', 'sign', 'simple', 'sing', 'singing', 'single',\n",
       "       'sister', 'sit', 'site', 'sitting', 'sleep', 'sleeping', 'sleepy',\n",
       "       'slept', 'slow', 'small', 'smile', 'snl', 'sold', 'son', 'song',\n",
       "       'soo', 'soon', 'sooo', 'soooo', 'sooooo', 'sore', 'sorry', 'sound',\n",
       "       'special', 'spend', 'spending', 'spent', 'st', 'stand', 'star',\n",
       "       'starbucks', 'start', 'started', 'starting', 'state', 'stay',\n",
       "       'stick', 'stomach', 'stop', 'stopped', 'store', 'story', 'strange',\n",
       "       'stressed', 'stuck', 'student', 'study', 'studying', 'stuff',\n",
       "       'stupid', 'suck', 'sucks', 'summer', 'sun', 'sunday', 'sunny',\n",
       "       'sunshine', 'super', 'support', 'supposed', 'sure', 'surgery',\n",
       "       'surprise', 'sux', 'swear', 'sweet', 'sweetie', 'swine', 'system',\n",
       "       'taken', 'taking', 'talent', 'talk', 'talking', 'taste', 'taylor',\n",
       "       'tea', 'team', 'tear', 'tell', 'terrible', 'test', 'text', 'th',\n",
       "       'thank', 'thanks', 'thanx', 'thats', 'theres', 'theyre', 'thing',\n",
       "       'things', 'think', 'thinking', 'tho', 'thought', 'throat',\n",
       "       'thursday', 'thx', 'ticket', 'til', 'till', 'time', 'times',\n",
       "       'tired', 'today', 'told', 'tom', 'tomorrow', 'tonight', 'took',\n",
       "       'totally', 'touch', 'tour', 'town', 'track', 'traffic', 'train',\n",
       "       'travel', 'trek', 'tried', 'trip', 'trouble', 'true', 'truly',\n",
       "       'try', 'trying', 'tuesday', 'tummy', 'turn', 'turned', 'tv',\n",
       "       'tweet', 'tweetdeck', 'tweeting', 'tweets', 'twilight', 'twitter',\n",
       "       'ugh', 'ugly', 'uk', 'understand', 'unfortunately', 'update',\n",
       "       'upload', 'upset', 'ur', 'use', 'usually', 'version', 'video',\n",
       "       'visit', 'voice', 'vote', 'wa', 'wait', 'waiting', 'wake',\n",
       "       'waking', 'walk', 'walking', 'wanna', 'want', 'wanted', 'war',\n",
       "       'warm', 'wasnt', 'watch', 'watched', 'watching', 'water', 'way',\n",
       "       'wear', 'wearing', 'weather', 'website', 'wedding', 'week',\n",
       "       'weekend', 'weeks', 'weird', 'welcome', 'went', 'whats', 'white',\n",
       "       'wife', 'win', 'window', 'wine', 'wish', 'wishing', 'woke',\n",
       "       'woman', 'won', 'wonder', 'wonderful', 'wondering', 'wont', 'woo',\n",
       "       'word', 'work', 'worked', 'working', 'world', 'worried', 'worry',\n",
       "       'worse', 'worst', 'worth', 'wouldnt', 'wow', 'write', 'writing',\n",
       "       'wrong', 'wtf', 'xd', 'xoxo', 'ya', 'yall', 'yay', 'yea', 'yeah',\n",
       "       'year', 'years', 'yep', 'yes', 'yesterday', 'yo', 'youll', 'young',\n",
       "       'youre', 'youtube', 'youve', 'yr', 'yum', 'yummy', '½m', '½s'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "84232a1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:08:36.848531Z",
     "start_time": "2022-06-07T13:08:36.766568Z"
    }
   },
   "outputs": [],
   "source": [
    "np_features = features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "50d5d571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:08:38.328208Z",
     "start_time": "2022-06-07T13:08:38.320072Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16363, 1000)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e3479",
   "metadata": {},
   "source": [
    "We should also work with the labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dfa5b4f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:09:21.017350Z",
     "start_time": "2022-06-07T13:09:20.981632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>num_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>sooo sad miss san diego</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>bos bullying</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>interview leave</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>son couldnt release bought</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>am feeding baby fun smile coo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text sentiment  \\\n",
       "0      1      Sooo SAD I will miss you here in San Diego!!!  negative   \n",
       "1      2                          my boss is bullying me...  negative   \n",
       "2      3                     what interview! leave me alone  negative   \n",
       "3      4   Sons of ****, why couldn`t they put them on t...  negative   \n",
       "4      6  2am feedings for the baby are fun when he is a...  positive   \n",
       "\n",
       "               preprocessed_text  num_sentiment  \n",
       "0        sooo sad miss san diego              0  \n",
       "1                   bos bullying              0  \n",
       "2                interview leave              0  \n",
       "3     son couldnt release bought              0  \n",
       "4  am feeding baby fun smile coo              1  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['num_sentiment'] = df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d7c9d5ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:09:30.734110Z",
     "start_time": "2022-06-07T13:09:30.729289Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = df['num_sentiment'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b257eefa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:09:33.567310Z",
     "start_time": "2022-06-07T13:09:33.559989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a7cf9",
   "metadata": {},
   "source": [
    "## Train / Test / Dev Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1ba0006a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:12:31.888186Z",
     "start_time": "2022-06-07T13:12:31.589910Z"
    }
   },
   "outputs": [],
   "source": [
    "f_train, f_rem, l_train, l_rem = train_test_split(np_features, labels, test_size=1-config['train_percentage']/100, random_state=50)\n",
    "f_test, f_dev, l_test, l_dev = train_test_split(f_rem, l_rem, test_size=0.5, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "55f1275e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:12:33.276538Z",
     "start_time": "2022-06-07T13:12:33.271927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train features: (14726, 1000), dev features: (819, 1000), test features: (818, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(f'train features: {f_train.shape}, dev features: {f_dev.shape}, test features: {f_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8d226c3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:12:51.450352Z",
     "start_time": "2022-06-07T13:12:51.437967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels: 14726, dev labels: 819, test labels: 818\n"
     ]
    }
   ],
   "source": [
    "print(f'train labels: {len(l_train)}, dev labels: {len(l_dev)}, test labels: {len(l_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd2ef51",
   "metadata": {},
   "source": [
    "## Converting Everything to Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45678ad",
   "metadata": {},
   "source": [
    "The numpy array we defined above should be converted to a tensor. This tensor will be used in a \"Dataset\" object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6b08c1fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:14:40.849936Z",
     "start_time": "2022-06-07T13:14:40.837745Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyVectorDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = np.array(labels).reshape(-1, 1)\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.Tensor(self.features[idx]), torch.Tensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "80e62889",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:14:54.781392Z",
     "start_time": "2022-06-07T13:14:54.775362Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = MyVectorDataset(f_train, l_train)\n",
    "test_dataset = MyVectorDataset(f_test, l_test)\n",
    "dev_dataset = MyVectorDataset(f_dev, l_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7e443816",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:15:09.393343Z",
     "start_time": "2022-06-07T13:15:09.378632Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=config['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5006d18",
   "metadata": {},
   "source": [
    "# Neural Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "070b61b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:15:46.833065Z",
     "start_time": "2022-06-07T13:15:46.817936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e9bdad21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:17:43.987486Z",
     "start_time": "2022-06-07T13:17:43.974767Z"
    }
   },
   "outputs": [],
   "source": [
    "class my_neural_net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(my_neural_net, self).__init__() \n",
    "        self.first_layer = torch.nn.Sequential( \n",
    "            nn.Linear(config['max_features'], 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        output = self.first_layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "01e8f9a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:17:46.903195Z",
     "start_time": "2022-06-07T13:17:46.874879Z"
    }
   },
   "outputs": [],
   "source": [
    "simple_nn = my_neural_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3d469445",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:18:01.254913Z",
     "start_time": "2022-06-07T13:18:01.235635Z"
    }
   },
   "outputs": [],
   "source": [
    "simple_nn = simple_nn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "eafc60c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:18:27.119810Z",
     "start_time": "2022-06-07T13:18:27.032854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4972],\n",
       "        [0.4993]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_nn(train_dataset[:2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "987951b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:18:41.231088Z",
     "start_time": "2022-06-07T13:18:41.215579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_nn(train_dataset[:2][0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2904153e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa42b77",
   "metadata": {},
   "source": [
    "## Binary Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68284027",
   "metadata": {},
   "source": [
    "For each point, the loss is calculated like this: $l_n = -w_n[y_n.\\log(\\mathrm{pred}_n)+(1-y_n).\\log(1-\\mathrm{pred}_n)]$ where $w_n$ is a rescaling factor</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b86a65",
   "metadata": {},
   "source": [
    "Assume that $w_n=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92294e91",
   "metadata": {},
   "source": [
    "If $y_n=0$ and $\\mathrm{pred}_n=1$, then we'll have $l_n=-w_n(0.log(1)+1.log(0))=-w_n(0.log(1)+1.-\\infty)=+\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05728e3f",
   "metadata": {},
   "source": [
    "If $y_n=0$ and $\\mathrm{pred}_n=0.1$, then we'll have $l_n=-w_n(0.log(0.1)+1.log(0.9))=-(-0.04)=0.04$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c16527",
   "metadata": {},
   "source": [
    "If $y_n=0$ and $\\mathrm{pred}_n=0.9$, then we'll have $l_n=-w_n(0.log(0.9)+1.log(0.1))=-(-1)=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c86608dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:21:25.439043Z",
     "start_time": "2022-06-07T13:21:25.428685Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b5ddc",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518ac0cf",
   "metadata": {},
   "source": [
    "Note that stochastic gradient descent performs a parameter update for each training example $x_i$ and label $y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484ba024",
   "metadata": {},
   "source": [
    "$\\theta = \\theta - \\eta.\\nabla_\\theta J(\\theta;x_i;y_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5f6fcd5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:23:13.579253Z",
     "start_time": "2022-06-07T13:23:13.565666Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(simple_nn.parameters(), lr=config['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "95de2f05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:24:13.074906Z",
     "start_time": "2022-06-07T13:24:13.063585Z"
    }
   },
   "outputs": [],
   "source": [
    "def output_to_label(out):\n",
    "    dist_to_0 = abs(out)\n",
    "    dist_to_1 = abs(out-1)\n",
    "    if dist_to_0 <= dist_to_1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bbd5fc91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:31:46.646317Z",
     "start_time": "2022-06-07T13:31:46.616928Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, epoch_num):\n",
    "    num_points = len(dataloader.dataset)\n",
    "    for batch, (features, labels) in enumerate(dataloader):        \n",
    "        # Compute prediction and loss\n",
    "        pred = model(features)\n",
    "        loss = loss_fn(pred, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # sets gradients of all model parameters to zero\n",
    "        loss.backward() # calculate the gradients again\n",
    "        optimizer.step() # w = w - learning_rate * grad(loss)_with_respect_to_w\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(features)\n",
    "            print(f\"\\r Epoch {epoch_num} - loss: {loss:>7f}  [{current:>5d}/{num_points:>5d}]\", end=\" \")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, epoch_num, name):\n",
    "    num_points = len(dataloader.dataset)\n",
    "    sum_test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (features, labels) in enumerate(dataloader):\n",
    "            pred = model(features)\n",
    "            sum_test_loss += loss_fn(pred, labels).item() # add the current loss to the sum of the losses\n",
    "            # convert the outputs of the model on the current batch to a numpy array\n",
    "            pred_lst = list(pred.numpy().squeeze())\n",
    "            pred_lst = [output_to_label(item) for item in pred_lst]\n",
    "            # convert the original labels corresponding to the current batch to a numpy array\n",
    "            output_lst = list(labels.numpy().squeeze()) \n",
    "            # determine the points for which the model is correctly predicting the label (add a 1 for each)\n",
    "            match_lst = [1 if p==o else 0 for (p, o) in zip(pred_lst, output_lst)] \n",
    "            # count how many points are labeled correctly in this batch and add the number to the overall count of the correct labeled points\n",
    "            correct += sum(match_lst) \n",
    "            \n",
    "    sum_test_loss /= num_points\n",
    "    correct /= num_points\n",
    "    print(f\"\\r Epoch {epoch_num} - {name} Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {sum_test_loss:>8f}\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2956a42e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:33:38.883705Z",
     "start_time": "2022-06-07T13:31:56.079832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 200 - Development/Validation Error: Accuracy: 86.1%, Avg loss: 0.010535 "
     ]
    }
   ],
   "source": [
    "for epoch_num in range(1, config['num_epochs']+1):\n",
    "    train_loop(train_dataloader, simple_nn, loss_fn, optimizer, epoch_num)\n",
    "    test_loop(dev_dataloader, simple_nn, loss_fn, epoch_num, 'Development/Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7cc97521",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:33:38.958639Z",
     "start_time": "2022-06-07T13:33:38.886807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " Epoch 200 - Test Error: Accuracy: 83.7%, Avg loss: 0.011854 "
     ]
    }
   ],
   "source": [
    "test_loop(test_dataloader, simple_nn, loss_fn, epoch_num, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1517b0",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3f0c4405",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:34:27.534587Z",
     "start_time": "2022-06-07T13:34:27.507954Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(simple_nn.state_dict(), \"neural_net.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4def5d",
   "metadata": {},
   "source": [
    "# Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b9feed95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:35:28.453060Z",
     "start_time": "2022-06-07T13:35:28.411372Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_neural_net(\n",
       "  (first_layer): Sequential(\n",
       "    (0): Linear(in_features=1000, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = my_neural_net()\n",
    "model.load_state_dict(torch.load(\"neural_net.pth\"))\n",
    "model.eval() # use this line if you have Dropout and BatchNormalization layers in your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "900fd5ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:35:33.428980Z",
     "start_time": "2022-06-07T13:35:33.412855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1337],\n",
       "        [0.9956]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_dataset[:2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c2659ab6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:35:41.701427Z",
     "start_time": "2022-06-07T13:35:41.690160Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_test[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0df351",
   "metadata": {},
   "source": [
    "# The ONNX Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e11893",
   "metadata": {},
   "source": [
    "This format is useful when you want to use your model while coding in Java, Javascript, and C#!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c98b040",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0e75ced9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:36:49.073992Z",
     "start_time": "2022-06-07T13:36:49.047633Z"
    }
   },
   "outputs": [],
   "source": [
    "dummy_input = torch.zeros((1,config['max_features']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3e259531",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:37:00.073765Z",
     "start_time": "2022-06-07T13:36:59.889922Z"
    }
   },
   "outputs": [],
   "source": [
    "onnx.export(model, dummy_input, 'neural_net.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6d5742",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d9639473",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:37:26.839286Z",
     "start_time": "2022-06-07T13:37:26.756831Z"
    }
   },
   "outputs": [],
   "source": [
    "session = onnxruntime.InferenceSession('neural_net.onnx', None) # None: we want all of the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b9ae9c59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:37:32.213835Z",
     "start_time": "2022-06-07T13:37:32.198022Z"
    }
   },
   "outputs": [],
   "source": [
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "97f068c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:37:34.609760Z",
     "start_time": "2022-06-07T13:37:34.599874Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'onnx::Gemm_0'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5104f885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:37:35.333370Z",
     "start_time": "2022-06-07T13:37:35.326109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "60e0d09d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:38:21.799264Z",
     "start_time": "2022-06-07T13:38:21.764289Z"
    }
   },
   "outputs": [],
   "source": [
    "result = session.run([output_name], {input_name: test_dataset[0][0].numpy().reshape(1,-1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "881770f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:38:23.073119Z",
     "start_time": "2022-06-07T13:38:23.051015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.1336531]], dtype=float32)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
